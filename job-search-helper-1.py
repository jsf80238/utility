import csv
import logging
import math
import re
import requests
import sys
import urllib.parse
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO, format="%(asctime)s | %(levelname)s | %(message)s", datefmt="%Y-%m-%d.%H:%M:%S")
BASE_SEARCH_STRING = "https://www.google.com/search?q="
PREFIX = "/url?q="
SUFFIX = "&"

extra_list = ("jobs", "careers", "search results")
link_text_pattern_list = (
    re.compile(r"search\s+results", re.IGNORECASE),
    re.compile(r"careers", re.IGNORECASE),
    re.compile(r"jobs", re.IGNORECASE),
)

employer_list = (
    "Humana",
    "Kimberly-Clark",
    "Fortinet",
    "Ametek",
    "Prudential Financial",
    "Lennar",
    "Centene",
    "Sysco",
    "Otis Worldwide",
    "Cencora",
    "Dow",
    "Dominion Energy",
    "L3harris Technologies",
    "Corteva",
    "Cummins",
    "General Mills",
    "Costar Group",
    "Republic Services",
    "Paychex",
    "Ingersoll Rand",
    "Ge Healthcare Technologies",
    "Old Dominion Freight Line",
    "Quanta Services",
    "Kroger",
    "Exelon",
    "Kenvue",
    "Moderna",
    "Vulcan Materials",
    "Pg & e",
    "Kinder Morgan",
    "Cdw",
    "Global Payments",
    "Ppg Industries",
    "Verisk Analytics",
    "Arch Capital Group Ltd",
    "Rockwell Automation",
    "Baker Hughes",
    "Public Service Enterprise Grouprporated",
    "Dupont De Nemours",
    "On Semiconductor",
    "Monolithic Power Systems",
    "Biogen",
    "Consolidated Edison",
    "Extra Space Storage",
    "Xylem",
    "Vici Properties",
    "Hartford Financial Services Group",
    "Fortive",
    "Ansys",
    "Xcel Energy",
    "West Pharmaceutical Services",
    "Hershey",
    "Kraft Heinz",
    "Mettler-Toledo International",
    "Resmed",
    "Cbre Group",
    "Willis Towers Watson Public Limited Companys",
    "Tractor Supply",
    "Zimmer Biomet Holdings",
    "Keysight Technologies",
    "Cardinal Health",
    "T Rowe Price Group",
    "Howmet Aerospace",
    "Edison International",
    "Weyerhaeuser",
    "Lyondellbasell Industries",
    "Wabtec",
    "Hp",
    "Builders Firstsource",
    "Avalonbay Communities",
    "Wec Energy Group",
    "Church & Dwight",
    "Nvr",
    "Pultegroup",
    "Corning",
    "Fifth Third Bancorp",
    "Dover",
    "Targa Resources",
    "Molina Healthcare",
    "Deckers Outdoor",
    "Nasdaq",
    "American Water Works",
    "Sba Communications",
    "State Street",
    "Iron Mountain",
    "Align Technology",
    "Take-Two Interactive Software",
    "Garmin Ltd",
    "Hewlett Packard Enterprise",
    "Ptc",
    "Steel Dynamics",
    "Steris",
    "Axon Enterprise",
    "Aptiv",
    "Hubbell",
    "Corpay",
    "Netapp",
    "International Flavors & Fragrances",
    "Illumina",
    "Genuine Parts",
    "Western Digital.",
    "Baxter International",
    "Equity Residential",
    "Ball",
    "Brown & Brown",
    "Waters",
    "Teledyne Technologiesrporated",
    "Cooper Companies",
    "Ppl",
    "Invitation Homes",
    "Alexandria Real Estate Equities",
    "Huntington Bancshares",
    "Veralto",
    "Cincinnati Financial",
    "Jacobs Solutions",
    "Regions Financial.",
    "Warner Bros.Discovery Series a",
    "Mccormick & Companyrporated Non-Vtg Cs",
    "Cboe Global Markets",
    "Ameren",
    "Clorox",
    "Textron",
    "Omnicom Group",
    "Expedia Group",
    "Idex",
    "Laboratory of America Holdings",
    "Seagate Technology Holdings Plcs",
    "Northern Trust",
    "Hologic",
    "Avery Dennison.",
    "W.R.Berkley",
    "First Solar",
    "Leidos Holdings",
    "Expeditors International of Washington",
    "Tyler Technologies",
    "Masco",
    "Synchrony Financial",
    "Southwest Airlines",
    "Jabil",
    "Ventas",
    "Carnival",
    "Skyworks Solutions",
    "Everest Group, Ltd.",
    "Teradyne",
    "Factset Research Systems",
    "Packaging of America",
    "Celanese",
    "Tyson Foods",
    "Verisign",
    "Citizens Financial Group",
    "Akamai Technologies",
    "Pool",
    "Jb Hunt Transport Services",
    "Cf Industries Holding",
    "Trimble",
    "Snap-onrporated",
    "Essex Property Trust",
    "Zebra Technologies",
    "Lamb Weston Holdings",
    "Mid-America Apartment Communities",
    "Live Nation Entertainment",
    "Albemarle",
    "Walgreens Boots Alliance",
    "Nrg Energy",
    "Kellanova",
    "Stanley Black & Decker",
    "Bunge Global Sa",
    "Eqt",
    "Nordson",
    "Keycorp",
    "Quest Diagnostics",
    "Lkq",
    "Viatris",
    "Conagra Brands",
    "Pentair",
    "Carmax",
    "Amcor Plcs",
    "International Paper",
    "Rollins",
    "Revvity",
    "Healthpeak Properties",
    "J.M.Smucker",
    "Kimco Realty.",
    "Interpublic Group of Companies",
    "Henry (Jack) & Associates",
    "Westrock",
    "Gen",
    "Digital",
    "Juniper",
    "Networks",
    "Eastman",
    "Chemical",
    "Insulet",
    "Allegion",
    "Public",
    "Limited",
    "Aes",
    "Huntington",
    "Ingalls",
    "Industries",
    "Udr",
    "F5",
    "Qorvo",
    "Nisource",
    "Universal",
    "Health",
    "Services",
    "Bio - Techne",
    "A.O.Smith",
    "Tapestry",
    "Incyte",
    "Genomics",
    "Teleflexrporated",
    "Mosaic",
    "Camden",
    "Property",
    "Trust",
    "Apa",
    "Catalent",
    "Dayforce",
    "Regency",
    "Centers",
    "Henry",
    "Schein",
    "Match",
    "Group",
    "Paycom",
    "Software",
    "Brown - Forman",
    "Boston",
    "Properties",
    "C.H.Robinson",
    "Worldwide",
    "Pinnacle",
    "West",
    "Capital",
    "Marketaxess",
    "Holdings",
    "Borgwarner",
    "Generac",
    "Holdings",
    "Federal",
    "Realty",
    "Investment",
    "Trust",
    "Bio - Rad",
    "Laboratories",
    "Hasbro",
    "Comerica",
    "Dentsply",
    "Sirona",
    "Mohawk",
    "Industries",
    "Paramount",
    "Global",
    "V.F.",
)


def search_employer(name: str) -> list:
    for extra in extra_list:
        full_search_string = BASE_SEARCH_STRING + urllib.parse.quote(name + " " + extra)
        response = requests.get(full_search_string)
        soup = BeautifulSoup(response.text, features="html.parser")
        for link in soup.find_all('a'):
            url_text = link.attrs['href']
            if url_text.startswith(PREFIX):
                url = url_text.split(PREFIX)[-1].split(SUFFIX)[0]
                for pattern in link_text_pattern_list:
                    if pattern.search(link.text):
                        return url
    raise Exception("No link found.")

if not employer_list:
    exit()
padding = 1 + int(math.log10(len(employer_list)))
result_list = list()
for i, employer_name in enumerate(employer_list, 1):
    item_number = str(i).rjust(padding)
    logging.info(f"Employer {item_number} of {len(employer_list)}: {employer_name}")
    try:
        result = search_employer(employer_name)
        result_list.append((employer_name, None, None, None, None, result))
    except Exception as e:
        logging.warning(e)
    if False and i >= 2:
        break

header = (
    "Company",
    "Recruiter",
    "Connections",
    "Updated",
    "Notes",
    "URL",
)
csvwriter = csv.writer(sys.stdout)
csvwriter.writerow(header)
for row in result_list:
    csvwriter.writerow(row)
